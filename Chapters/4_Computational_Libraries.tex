\chapter{Domain Specific Languages for Parallel Computing}

%\section{Brief overview of modern Parallel Computing}

\section{OpenMP 4.0/4.5 in Clang and LLVM}


\subsection{OpenMP new concepts}

%Introduzione OpenMP and High Performance Community
OpenMP API has been massively used by the High Performance Community in the last years. In fact, OpenMP requires little programming effort to achieve good performance and an high portability across shared memory architectures from different vendors. Originally, OpenMP was mainly created to exploit traditional CPU technology, nevertheless nowadays high performance accelerators, like GPUs, Xeon Phi and FPGA, have increasingly leveraged to increase performance on data-parallel applications. This tendency is due to fact that a single accelerator can be 100 times faster than a traditional CPU. These modern accelerations are characterized by a massively parallel processing capabilities, containing around $6,000$ cores respect to only 32 cores in todayâ€™s most powerful CPUs. For example, NVIDIA V100, powered by Volta architecture, is equipped with $5,120$ Cuda cores and $640$ tensor cores and capable of reaching a peak of 7.5 TFLOPS on double-precision processing. % introdurre memoria?? 

%riformulare
In general, to obtain significant accelerations on current generations of high performance devices, HPC programmers use different degree of parallelism, usually by hand-tuning their code, performing architecture-specific transformations or using domain-specific libraries and languages. This development can be time consuming and requires a big effort from the HPC programmer.

To solve this problem different researchers, developed by many research center across EU and US, has been demonstrated that one solution is to utilize a high-level abstractions (HLA) development strategy based on Embedded Domain Specific Languages (EDSLs). The aim is to simplify the development, separating what has to be computed from how is computed, totally hiding how the parallel code is implemented. 

For this reason, the new OpenMP 4.0 has been released on July 2013, adding support for accelerations for the first time. 
This new release has introduced new compiler directives and library routines, making easy execute the computation on modern HPC devices, respect to the low-level programming language like CUDA and OpenCL, where the HPC programmer is language dependent and has to specify different information in order to exploit the accelerator. Although, the new OpenMP \emph{target} construct allows specifying the code region and the data to be executed on the compliant device. The data-mapping is completely seamless, the programmer can transfer the data specifying only the new \emph{map} construct, delegating the transfer directly to the embedded implementation. The following code shows an example of OpenMP 4.0/4.5:

\lstinputlisting[language=Octave]{Chapters/code/openmp4.c}

Here, in particular, the new \emph{teams} construct creates a league of thread teams(i.e., CUDA blocks) and the master thread of each team executes the region, while the\emph{ distribute}
directive distributes the iterations to the master thread of each team.

%cit OpenCAL and OPS, OP2	

\subsection{The logic behind the driver}

\subsection{Runtime library for generic offloading and Nvidia GPUs}

\subsection{Code Generation}


\section{OpenCAL}

\section{OPS}